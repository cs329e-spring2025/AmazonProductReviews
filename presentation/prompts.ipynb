{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is prompt one where we decided too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the given product image and extract relevant metadata attributes. Identify and categorize unique features, colors, and text present in the image.\n",
    "\n",
    "Extract the following attributes (if applicable):\n",
    "Product Identification: Extract product_id, ASIN, UPC (if visible).\n",
    "Brand & Naming: Identify brand_name, product_name, model_number.\n",
    "Categorization: Identify Main_Category, Subcategory_1, Subcategory_2, Subcategory_3.\n",
    "Pricing Details: If visible, extract list_price and selling_price.\n",
    "Technical Details: Extract dimensions, weight, product_specs, product_technical information.\n",
    "Text Extraction: If the image contains any text, extract the words accurately.\n",
    "Visual Features: Identify relevant object features such as materials, packaging details, logos, and labels.\n",
    "Color Information: Identify dominant colors present in the image.\n",
    "\n",
    "Return the output in JSON format.\n",
    "\n",
    "{\n",
    "\"business\": \"product_data_analysis\",\n",
    "\"category\": \"product_metadata\",\n",
    "\"product_attributes\": {\n",
    "\"product_id\": \"STRING\",\n",
    "\"brand_name\": \"STRING\",\n",
    "\"product_name\": \"STRING\",\n",
    "\"ASIN\": \"STRING\",\n",
    "\"UPC\": \"BOOL\",\n",
    "\"Main_Category\": \"STRING\",\n",
    "\"Subcategory_1\": \"STRING\",\n",
    "\"Subcategory_2\": \"STRING\",\n",
    "\"Subcategory_3\": \"STRING\",\n",
    "\"list_price\": \"FLOAT\",\n",
    "\"selling_price\": \"FLOAT\",\n",
    "\"quantity\": \"INT\",\n",
    "\"model_number\": \"STRING\",\n",
    "\"about_product\": \"STRING\",\n",
    "\"product_specs\": \"STRING\",\n",
    "\"product_technical\": \"STRING\",\n",
    "\"weight\": \"STRING\",\n",
    "\"dimensions\": \"STRING\",\n",
    "\"url\": \"STRING\"\n",
    "},\n",
    "\"image_analysis\": {\n",
    "\"detected_text\": [\"TEXT\"],\n",
    "\"dominant_colors\": [\"COLOR1\", \"COLOR2\"],\n",
    "\"object_features\": [\"FEATURE1\", \"FEATURE2\"]\n",
    "}\n",
    "}\n",
    "\n",
    "Do not include any extra details outside this format. Ensure extracted attributes are precise and relevant to the specific product image.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "from google.cloud import storage\n",
    "from io import StringIO\n",
    "\n",
    "# Configuration\n",
    "project_id = \"amazon-product-reviews-452322\"\n",
    "region = \"us-central1\"\n",
    "bucket_name = \"product_data_284725\"\n",
    "gcs_in_folder = \"initial_loads/product-image-data/in\"\n",
    "gcs_out_folder = \"initial_loads/product-image-data/out\"\n",
    "final_csv_name = \"product-image-data-all.csv\"\n",
    "model_name = \"gemini-1.5-flash-002\"\n",
    "\n",
    "# Enhanced prompt for image analysis with new requested attributes\n",
    "prompt = \"\"\"\n",
    "Analyze the given product image and extract relevant metadata attributes. Identify and categorize unique features, colors, and text present in the image.\n",
    "\n",
    "Extract the following attributes (if applicable):\n",
    "- Brand & Naming: Extract brand_name, product_name, model_number.\n",
    "- Categorization: Identify Main_Category, Subcategory_1, Subcategory_2, Subcategory_3.\n",
    "- Pricing Details: If visible, extract list_price and selling_price.\n",
    "- Technical Details: Extract product_specs, product_technical information.\n",
    "- Text Extraction: If the image contains any text, extract the words accurately.\n",
    "- Visual Features: Identify relevant object features such as materials, packaging details, logos, and labels.\n",
    "- Color Information: Identify dominant colors present in the image.\n",
    "- Logo Detection: Recognize brand logos if visible.\n",
    "- Text Sentiment: Analyze text on packaging for positive or negative language.\n",
    "- Object Detection: Identify key objects (e.g., accessories, packaging).\n",
    "- Image Quality: Assess clarity, lighting conditions, and presence of watermarks.\n",
    "\n",
    "Return the output in JSON format.\n",
    "\n",
    "{\n",
    "\"business\": \"product_data_analysis\",\n",
    "\"category\": \"product_metadata\",\n",
    "\"product_attributes\": {\n",
    "    \"brand_name\": \"STRING\",\n",
    "    \"product_name\": \"STRING\",\n",
    "    \"Main_Category\": \"STRING\",\n",
    "    \"Subcategory_1\": \"STRING\",\n",
    "    \"Subcategory_2\": \"STRING\",\n",
    "    \"Subcategory_3\": \"STRING\",\n",
    "    \"list_price\": \"FLOAT\",\n",
    "    \"selling_price\": \"FLOAT\",\n",
    "    \"model_number\": \"STRING\",\n",
    "    \"about_product\": \"STRING\",\n",
    "    \"product_specs\": \"STRING\",\n",
    "    \"product_technical\": \"STRING\",\n",
    "    \"url\": \"STRING\"\n",
    "},\n",
    "\"image_analysis\": {\n",
    "    \"detected_text\": [\"TEXT\"],\n",
    "    \"dominant_colors\": [\"COLOR1\", \"COLOR2\"],\n",
    "    \"object_features\": [\"FEATURE1\", \"FEATURE2\"],\n",
    "    \"logo_detection\": [\"LOGO1\", \"LOGO2\"],\n",
    "    \"text_sentiment\": \"STRING\",\n",
    "    \"object_detection\": [\"OBJECT1\", \"OBJECT2\"],\n",
    "    \"image_quality\": {\n",
    "        \"clarity\": \"STRING\",\n",
    "        \"lighting\": \"STRING\",\n",
    "        \"watermarks\": \"BOOL\"\n",
    "    }\n",
    "}\n",
    "}\n",
    "\n",
    "Do not include any extra details outside this format. Ensure extracted attributes are precise and relevant to the specific product image.\n",
    "\"\"\"\n",
    "\n",
    "def process_product_images():\n",
    "    \"\"\"Process product images from GCS bucket using Vertex AI's Generative AI\"\"\"\n",
    "    # Initialize Vertex AI\n",
    "    vertexai.init(project=project_id, location=region)\n",
    "    model = GenerativeModel(model_name)\n",
    "\n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List all files in the input folder\n",
    "    blobs = list(bucket.list_blobs(prefix=gcs_in_folder))\n",
    "\n",
    "    print(f\"Found {len(blobs)} files in {gcs_in_folder}\")\n",
    "\n",
    "    # Debug: Print first few blobs to understand the structure\n",
    "    for i, blob in enumerate(blobs):\n",
    "        if i < 5:  # Print first 5 blobs\n",
    "            print(f\"Debug - Blob {i}: {blob.name}\")\n",
    "\n",
    "    # Initialize a list to store all product data for the final consolidated CSV\n",
    "    all_product_data = []\n",
    "\n",
    "    # Define CSV headers based on the enhanced prompt\n",
    "    headers = [\n",
    "        \"product_id\", \"brand_name\", \"product_name\", \"Main_Category\", \n",
    "        \"Subcategory_1\", \"Subcategory_2\", \"Subcategory_3\",\n",
    "        \"list_price\", \"selling_price\", \"model_number\",\n",
    "        \"about_product\", \"product_specs\", \"product_technical\",\n",
    "        \"url\", \"detected_text\", \"dominant_colors\", \"object_features\",\n",
    "        \"logo_detection\", \"text_sentiment\", \"object_detection\",\n",
    "        \"image_quality_clarity\", \"image_quality_lighting\", \"image_quality_watermarks\",\n",
    "        \"image_path\"  # Added image path for reference\n",
    "    ]\n",
    "\n",
    "    for blob in blobs:\n",
    "        # Skip directories or non-image files\n",
    "        if blob.name.endswith('/') or not any(blob.name.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png']):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing: {blob.name}\")\n",
    "\n",
    "        try:\n",
    "            # Parse product information from filename\n",
    "            filename = blob.name.split(\"/\")[-1]\n",
    "            parts = filename.split(\"-\")\n",
    "\n",
    "            # Extract product ID from filename (if available)\n",
    "            product_id = parts[1] if len(parts) > 1 else \"unknown\"\n",
    "\n",
    "            # Create CSV output for this image\n",
    "            output_content = StringIO()\n",
    "            writer = csv.writer(output_content)\n",
    "\n",
    "            # Write header\n",
    "            writer.writerow(headers[:-1])  # Exclude image_path from individual files\n",
    "\n",
    "            # Process image with Gemini\n",
    "            image_uri = f\"gs://{bucket_name}/{blob.name}\"\n",
    "            image_part = Part.from_uri(image_uri, mime_type=\"image/jpeg\")\n",
    "\n",
    "            response = model.generate_content([image_part, prompt])\n",
    "            response_text = response.text\n",
    "\n",
    "            # Clean up the JSON response\n",
    "            clean_json = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "            try:\n",
    "                # Parse the JSON response\n",
    "                result = json.loads(clean_json)\n",
    "\n",
    "                # Extract product attributes and image analysis\n",
    "                product_attrs = result.get(\"product_attributes\", {})\n",
    "                image_analysis = result.get(\"image_analysis\", {})\n",
    "                image_quality = image_analysis.get(\"image_quality\", {})\n",
    "\n",
    "                # Prepare row data with new attributes\n",
    "                row_data = [\n",
    "                    product_id,\n",
    "                    product_attrs.get(\"brand_name\", \"\"),\n",
    "                    product_attrs.get(\"product_name\", \"\"),\n",
    "                    product_attrs.get(\"Main_Category\", \"\"),\n",
    "                    product_attrs.get(\"Subcategory_1\", \"\"),\n",
    "                    product_attrs.get(\"Subcategory_2\", \"\"),\n",
    "                    product_attrs.get(\"Subcategory_3\", \"\"),\n",
    "                    product_attrs.get(\"list_price\", \"\"),\n",
    "                    product_attrs.get(\"selling_price\", \"\"),\n",
    "                    product_attrs.get(\"model_number\", \"\"),\n",
    "                    product_attrs.get(\"about_product\", \"\"),\n",
    "                    product_attrs.get(\"product_specs\", \"\"),\n",
    "                    product_attrs.get(\"product_technical\", \"\"),\n",
    "                    product_attrs.get(\"url\", \"\"),\n",
    "                    \"|\".join(image_analysis.get(\"detected_text\", [])),\n",
    "                    \"|\".join(image_analysis.get(\"dominant_colors\", [])),\n",
    "                    \"|\".join(image_analysis.get(\"object_features\", [])),\n",
    "                    \"|\".join(image_analysis.get(\"logo_detection\", [])),\n",
    "                    image_analysis.get(\"text_sentiment\", \"\"),\n",
    "                    \"|\".join(image_analysis.get(\"object_detection\", [])),\n",
    "                    image_quality.get(\"clarity\", \"\"),\n",
    "                    image_quality.get(\"lighting\", \"\"),\n",
    "                    str(image_quality.get(\"watermarks\", False))\n",
    "                ]\n",
    "\n",
    "                # Add to the consolidated data with image path\n",
    "                all_product_data.append(row_data + [blob.name])\n",
    "\n",
    "                # Write the data row to individual CSV\n",
    "                writer.writerow(row_data)\n",
    "\n",
    "                # Create output file name\n",
    "                output_filename = f\"{product_id}.csv\" if product_id != \"unknown\" else blob.name.split(\"/\")[-1].replace(\".jpg\", \".csv\").replace(\".jpeg\", \".csv\").replace(\".png\", \".csv\")\n",
    "                output_path = f\"{gcs_out_folder}/{output_filename}\"\n",
    "\n",
    "                # Save to GCS\n",
    "                output_blob = bucket.blob(output_path)\n",
    "                output_blob.upload_from_string(output_content.getvalue())\n",
    "\n",
    "                print(f\"Saved analysis for {blob.name} to {output_path}\")\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON response for {blob.name}: {e}\")\n",
    "                print(f\"Raw response: {clean_json}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {blob.name}: {e}\")\n",
    "\n",
    "    # Create the final consolidated CSV\n",
    "    if all_product_data:\n",
    "        create_consolidated_csv(bucket, headers, all_product_data)\n",
    "\n",
    "def create_consolidated_csv(bucket, headers, all_product_data):\n",
    "    \"\"\"Create a consolidated CSV with all product data\"\"\"\n",
    "    print(f\"Creating consolidated CSV with {len(all_product_data)} product entries\")\n",
    "\n",
    "    # Create the consolidated CSV content\n",
    "    output_content = StringIO()\n",
    "    writer = csv.writer(output_content)\n",
    "\n",
    "    # Write header\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    # Write all data rows\n",
    "    for row in all_product_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "    # Save the consolidated CSV to GCS\n",
    "    final_path = f\"{gcs_out_folder}/{final_csv_name}\"\n",
    "    output_blob = bucket.blob(final_path)\n",
    "    output_blob.upload_from_string(output_content.getvalue())\n",
    "\n",
    "    print(f\"Consolidated CSV saved to: {final_path}\")\n",
    "\n",
    "def list_all_folders():\n",
    "    \"\"\"List all folders in the bucket to help identify the correct path\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List all blobs in the bucket\n",
    "    blobs = list(bucket.list_blobs())\n",
    "\n",
    "    # Extract unique folder paths\n",
    "    folders = set()\n",
    "    for blob in blobs:\n",
    "        # Split the path and collect all parent folders\n",
    "        parts = blob.name.split('/')\n",
    "        for i in range(1, len(parts)):\n",
    "            folders.add('/'.join(parts[:i]))\n",
    "\n",
    "    # Print all folders\n",
    "    print(\"Available folders in the bucket:\")\n",
    "    for folder in sorted(folders):\n",
    "        print(f\"  - {folder}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # First list available folders to help identify the correct path\n",
    "    list_all_folders()\n",
    "\n",
    "    # Then process the images\n",
    "    process_product_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "from google.cloud import storage\n",
    "from io import StringIO\n",
    "import datetime\n",
    "\n",
    "# Configuration\n",
    "project_id = \"amazon-product-reviews-452322\"\n",
    "region = \"us-central1\"\n",
    "bucket_name = \"product_data_284725\"\n",
    "gcs_in_folder = \"initial_loads/product-image-data/in\"\n",
    "gcs_out_folder = \"initial_loads/product-image-data/out\"\n",
    "final_csv_name = f\"product-image-data-all-{datetime.datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "model_name = \"gemini-1.5-flash-002\"\n",
    "\n",
    "# Enhanced prompt for image analysis with adjusted categories and removed fields\n",
    "prompt = \"\"\"\n",
    "Analyze the given product image and extract relevant metadata attributes. Identify and categorize unique features, colors, and text present in the image.\n",
    "\n",
    "Extract the following attributes (if applicable):\n",
    "- Brand & Naming: Extract brand_name, product_name.\n",
    "- Categorization: Identify Main_Category, Subcategory_1, Subcategory_2, Subcategory_3, Subcategory_4, Subcategory_5.\n",
    "- Pricing Details: If visible, extract list_price and selling_price.\n",
    "- Technical Details: Extract product_technical information.\n",
    "- Text Extraction: If the image contains any text, extract the words accurately.\n",
    "- Visual Features: Identify relevant object features such as materials, packaging details, logos, and labels.\n",
    "- Color Information: Identify dominant colors present in the image.\n",
    "- Logo Detection: Recognize brand logos if visible.\n",
    "- Text Sentiment: Analyze text on packaging for positive or negative language.\n",
    "- Object Detection: Identify key objects (e.g., accessories, packaging).\n",
    "- Image Quality: Assess clarity, lighting conditions, and presence of watermarks.\n",
    "- Product Condition: Assess whether the product appears new, used, or refurbished.\n",
    "- Packaging Type: Identify retail box, bulk packaging, eco-friendly packaging, etc.\n",
    "- Target Demographic: Identify likely target audience based on visual cues.\n",
    "- Competitor Products: Identify if competitor products are visible in the image.\n",
    "\n",
    "Return the output in JSON format.\n",
    "\n",
    "{\n",
    "\"business\": \"product_data_analysis\",\n",
    "\"category\": \"product_metadata\",\n",
    "\"product_attributes\": {\n",
    "    \"brand_name\": \"STRING\",\n",
    "    \"product_name\": \"STRING\",\n",
    "    \"Main_Category\": \"STRING\",\n",
    "    \"Subcategory_1\": \"STRING\",\n",
    "    \"Subcategory_2\": \"STRING\",\n",
    "    \"Subcategory_3\": \"STRING\",\n",
    "    \"Subcategory_4\": \"STRING\",\n",
    "    \"Subcategory_5\": \"STRING\",\n",
    "    \"list_price\": \"FLOAT\",\n",
    "    \"selling_price\": \"FLOAT\",\n",
    "    \"about_product\": \"STRING\",\n",
    "    \"product_technical\": \"STRING\",\n",
    "    \"url\": \"STRING\"\n",
    "},\n",
    "\"image_analysis\": {\n",
    "    \"detected_text\": [\"TEXT\"],\n",
    "    \"dominant_colors\": [\"COLOR1\", \"COLOR2\"],\n",
    "    \"object_features\": [\"FEATURE1\", \"FEATURE2\"],\n",
    "    \"logo_detection\": [\"LOGO1\", \"LOGO2\"],\n",
    "    \"text_sentiment\": \"STRING\",\n",
    "    \"object_detection\": [\"OBJECT1\", \"OBJECT2\"],\n",
    "    \"image_quality\": {\n",
    "        \"clarity\": \"STRING\",\n",
    "        \"lighting\": \"STRING\",\n",
    "        \"watermarks\": \"BOOL\"\n",
    "    },\n",
    "    \"product_condition\": \"STRING\",\n",
    "    \"packaging_type\": \"STRING\",\n",
    "    \"target_demographic\": [\"DEMOGRAPHIC1\", \"DEMOGRAPHIC2\"],\n",
    "    \"competitor_products\": [\"COMPETITOR1\", \"COMPETITOR2\"]\n",
    "}\n",
    "}\n",
    "\n",
    "Do not include any extra details outside this format. Ensure extracted attributes are precise and relevant to the specific product image.\n",
    "\"\"\"\n",
    "\n",
    "def process_product_images():\n",
    "    \"\"\"Process product images from GCS bucket using Vertex AI's Generative AI\"\"\"\n",
    "    # Initialize Vertex AI\n",
    "    vertexai.init(project=project_id, location=region)\n",
    "    model = GenerativeModel(model_name)\n",
    "\n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List all files in the input folder\n",
    "    blobs = list(bucket.list_blobs(prefix=gcs_in_folder))\n",
    "\n",
    "    print(f\"Found {len(blobs)} files in {gcs_in_folder}\")\n",
    "\n",
    "    # Initialize a list to store all product data for the final consolidated CSV\n",
    "    all_product_data = []\n",
    "\n",
    "    # Define CSV headers based on the enhanced prompt\n",
    "    headers = [\n",
    "        \"product_id\", \"brand_name\", \"product_name\", \"Main_Category\", \n",
    "        \"Subcategory_1\", \"Subcategory_2\", \"Subcategory_3\", \"Subcategory_4\", \"Subcategory_5\",\n",
    "        \"list_price\", \"selling_price\",\n",
    "        \"about_product\", \"product_technical\",\n",
    "        \"url\", \"detected_text\", \"dominant_colors\", \"object_features\",\n",
    "        \"logo_detection\", \"text_sentiment\", \"object_detection\",\n",
    "        \"image_quality_clarity\", \"image_quality_lighting\", \"image_quality_watermarks\",\n",
    "        \"product_condition\", \"packaging_type\", \"target_demographic\", \"competitor_products\",\n",
    "        \"image_path\", \"analysis_timestamp\"  # Added timestamps for tracking\n",
    "    ]\n",
    "\n",
    "    # Track processing statistics\n",
    "    stats = {\n",
    "        \"total_images\": 0,\n",
    "        \"successful_analyses\": 0,\n",
    "        \"failed_analyses\": 0,\n",
    "        \"start_time\": datetime.datetime.now()\n",
    "    }\n",
    "\n",
    "    for blob in blobs:\n",
    "        # Skip directories or non-image files\n",
    "        if blob.name.endswith('/') or not any(blob.name.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png']):\n",
    "            continue\n",
    "\n",
    "        stats[\"total_images\"] += 1\n",
    "        print(f\"Processing {stats['total_images']}/{len(blobs)}: {blob.name}\")\n",
    "\n",
    "        try:\n",
    "            # Parse product information from filename\n",
    "            filename = blob.name.split(\"/\")[-1]\n",
    "            parts = filename.split(\"-\")\n",
    "\n",
    "            # Extract product ID from filename (if available)\n",
    "            product_id = parts[1] if len(parts) > 1 else \"unknown\"\n",
    "\n",
    "            # Create CSV output for this image\n",
    "            output_content = StringIO()\n",
    "            writer = csv.writer(output_content)\n",
    "\n",
    "            # Write header\n",
    "            writer.writerow(headers[:-2])  # Exclude image_path and timestamp from individual files\n",
    "\n",
    "            # Process image with Gemini\n",
    "            image_uri = f\"gs://{bucket_name}/{blob.name}\"\n",
    "            image_part = Part.from_uri(image_uri, mime_type=\"image/jpeg\")\n",
    "\n",
    "            response = model.generate_content([image_part, prompt])\n",
    "            response_text = response.text\n",
    "\n",
    "            # Clean up the JSON response\n",
    "            clean_json = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "            try:\n",
    "                # Parse the JSON response\n",
    "                result = json.loads(clean_json)\n",
    "\n",
    "                # Extract product attributes and image analysis\n",
    "                product_attrs = result.get(\"product_attributes\", {})\n",
    "                image_analysis = result.get(\"image_analysis\", {})\n",
    "                image_quality = image_analysis.get(\"image_quality\", {})\n",
    "\n",
    "                # Current timestamp\n",
    "                timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "                # Prepare row data with new attributes\n",
    "                row_data = [\n",
    "                    product_id,\n",
    "                    product_attrs.get(\"brand_name\", \"\"),\n",
    "                    product_attrs.get(\"product_name\", \"\"),\n",
    "                    product_attrs.get(\"Main_Category\", \"\"),\n",
    "                    product_attrs.get(\"Subcategory_1\", \"\"),\n",
    "                    product_attrs.get(\"Subcategory_2\", \"\"),\n",
    "                    product_attrs.get(\"Subcategory_3\", \"\"),\n",
    "                    product_attrs.get(\"Subcategory_4\", \"\"),\n",
    "                    product_attrs.get(\"Subcategory_5\", \"\"),\n",
    "                    product_attrs.get(\"list_price\", \"\"),\n",
    "                    product_attrs.get(\"selling_price\", \"\"),\n",
    "                    product_attrs.get(\"about_product\", \"\"),\n",
    "                    product_attrs.get(\"product_technical\", \"\"),\n",
    "                    product_attrs.get(\"url\", \"\"),\n",
    "                    \"|\".join(image_analysis.get(\"detected_text\", [])),\n",
    "                    \"|\".join(image_analysis.get(\"dominant_colors\", [])),\n",
    "                    \"|\".join(image_analysis.get(\"object_features\", [])),\n",
    "                    \"|\".join(image_analysis.get(\"logo_detection\", [])),\n",
    "                    image_analysis.get(\"text_sentiment\", \"\"),\n",
    "                    \"|\".join(image_analysis.get(\"object_detection\", [])),\n",
    "                    image_quality.get(\"clarity\", \"\"),\n",
    "                    image_quality.get(\"lighting\", \"\"),\n",
    "                    str(image_quality.get(\"watermarks\", False)),\n",
    "                    image_analysis.get(\"product_condition\", \"\"),\n",
    "                    image_analysis.get(\"packaging_type\", \"\"),\n",
    "                    \"|\".join(image_analysis.get(\"target_demographic\", [])),\n",
    "                    \"|\".join(image_analysis.get(\"competitor_products\", []))\n",
    "                ]\n",
    "\n",
    "                # Add to the consolidated data with image path and timestamp\n",
    "                all_product_data.append(row_data + [blob.name, timestamp])\n",
    "\n",
    "                # Write the data row to individual CSV\n",
    "                writer.writerow(row_data)\n",
    "\n",
    "                # Create output file name\n",
    "                output_filename = f\"{product_id}.csv\" if product_id != \"unknown\" else blob.name.split(\"/\")[-1].replace(\".jpg\", \".csv\").replace(\".jpeg\", \".csv\").replace(\".png\", \".csv\")\n",
    "                output_path = f\"{gcs_out_folder}/{output_filename}\"\n",
    "\n",
    "                # Save to GCS\n",
    "                output_blob = bucket.blob(output_path)\n",
    "                output_blob.upload_from_string(output_content.getvalue())\n",
    "\n",
    "                print(f\"Saved analysis for {blob.name} to {output_path}\")\n",
    "                stats[\"successful_analyses\"] += 1\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON response for {blob.name}: {e}\")\n",
    "                print(f\"Raw response: {clean_json}\")\n",
    "                stats[\"failed_analyses\"] += 1\n",
    "\n",
    "                # Save the raw response for debugging\n",
    "                error_path = f\"{gcs_out_folder}/errors/{blob.name.split('/')[-1]}.error.txt\"\n",
    "                error_blob = bucket.blob(error_path)\n",
    "                error_blob.upload_from_string(f\"Error: {str(e)}\\n\\nRaw response:\\n{clean_json}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {blob.name}: {e}\")\n",
    "            stats[\"failed_analyses\"] += 1\n",
    "\n",
    "    # Create the final consolidated CSV\n",
    "    if all_product_data:\n",
    "        create_consolidated_csv(bucket, headers, all_product_data)\n",
    "\n",
    "    # Calculate and print processing statistics\n",
    "    stats[\"end_time\"] = datetime.datetime.now()\n",
    "    stats[\"duration\"] = stats[\"end_time\"] - stats[\"start_time\"]\n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Total images processed: {stats['total_images']}\")\n",
    "    print(f\"Successful analyses: {stats['successful_analyses']}\")\n",
    "    print(f\"Failed analyses: {stats['failed_analyses']}\")\n",
    "    print(f\"Success rate: {(stats['successful_analyses']/stats['total_images'])*100 if stats['total_images'] > 0 else 0:.2f}%\")\n",
    "    print(f\"Processing time: {stats['duration']}\")\n",
    "\n",
    "    # Save statistics to GCS\n",
    "    stats_blob = bucket.blob(f\"{gcs_out_folder}/processing_stats_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "    stats_blob.upload_from_string(json.dumps(stats, default=str))\n",
    "\n",
    "def create_consolidated_csv(bucket, headers, all_product_data):\n",
    "    \"\"\"Create a consolidated CSV with all product data\"\"\"\n",
    "    print(f\"Creating consolidated CSV with {len(all_product_data)} product entries\")\n",
    "\n",
    "    # Create the consolidated CSV content\n",
    "    output_content = StringIO()\n",
    "    writer = csv.writer(output_content)\n",
    "\n",
    "    # Write header\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    # Write all data rows\n",
    "    for row in all_product_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "    # Save the consolidated CSV to GCS\n",
    "    final_path = f\"{gcs_out_folder}/{final_csv_name}\"\n",
    "    output_blob = bucket.blob(final_path)\n",
    "    output_blob.upload_from_string(output_content.getvalue())\n",
    "\n",
    "    print(f\"Consolidated CSV saved to: {final_path}\")\n",
    "\n",
    "def list_all_folders():\n",
    "    \"\"\"List all folders in the bucket to help identify the correct path\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List all blobs in the bucket\n",
    "    blobs = list(bucket.list_blobs())\n",
    "\n",
    "    # Extract unique folder paths\n",
    "    folders = set()\n",
    "    for blob in blobs:\n",
    "        # Split the path and collect all parent folders\n",
    "        parts = blob.name.split('/')\n",
    "        for i in range(1, len(parts)):\n",
    "            folders.add('/'.join(parts[:i]))\n",
    "\n",
    "    # Print all folders\n",
    "    print(\"Available folders in the bucket:\")\n",
    "    for folder in sorted(folders):\n",
    "        print(f\"  - {folder}\")\n",
    "\n",
    "def batch_processing(batch_size=10):\n",
    "    \"\"\"Process images in smaller batches to avoid memory issues\"\"\"\n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List all files in the input folder\n",
    "    blobs = list(bucket.list_blobs(prefix=gcs_in_folder))\n",
    "    image_blobs = [blob for blob in blobs if any(blob.name.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png'])]\n",
    "    \n",
    "    print(f\"Found {len(image_blobs)} image files to process\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(image_blobs), batch_size):\n",
    "        batch = image_blobs[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(image_blobs)-1)//batch_size + 1} ({len(batch)} images)\")\n",
    "        # Process the batch\n",
    "        # Implementation of batch processing would go here\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # First list available folders to help identify the correct path\n",
    "    list_all_folders()\n",
    "\n",
    "    # Then process the images\n",
    "    process_product_images()\n",
    "    \n",
    "    # Alternative: Use batch processing for large datasets\n",
    "    # batch_processing(batch_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
